{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from numpy.linalg import inv\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources for IWLS:\n",
    "# https://datascience.stackexchange.com/questions/75166/iterative-reweighted-least-squares-in-python\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb\n",
    "\n",
    "# Sources for GD & SGD:\n",
    "# https://stackoverflow.com/questions/47795918/logistic-regression-gradient-descent\n",
    "# https://medium.com/analytics-vidhya/gradient-descent-and-stochastic-gradient-descent-from-scratch-python-1cd93d4def49\n",
    "# https://github.com/Darshansol9/GD-SGD_FromScratch_Python/blob/master/Code.ipynb\n",
    "\n",
    "# Sources for ADAM:\n",
    "# https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d\n",
    "# https://github.com/jiexunsee/Adam-Optimizer-from-scratch/blob/master/ADAM.ipynb\n",
    "# https://machinelearningmastery.com/adam-optimization-from-scratch/\n",
    "\n",
    "# Source for cross-validation:\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers:\n",
    "    \n",
    "    def sigmoid(self,x:float)->float:\n",
    "        \"\"\" Activation function used to map any real value between 0 and 1 \"\"\"\n",
    "        # Activation function used to map any real value between 0 and 1\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def cost_function(self,X:np.matrix,y:np.array,params:np.array,b:np.array=None)->float:\n",
    "        \"\"\"\n",
    "        Computes the cost function for all the training samples \n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        \"\"\"\n",
    "        # Computes the cost function for all the training samples\n",
    "        fx = self.sigmoid(X.dot(params))\n",
    "        cost = -np.sum(y * np.log(fx) + (1 - y)* np.log(1-fx))\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(self,X:np.matrix,y:np.array,params:np.array,iterations:int,alpha:float)->np.array:\n",
    "        \"\"\"\n",
    "        Performs gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        \"\"\"\n",
    "        for i in range(iterations):\n",
    "            params = params + alpha * (X.T.dot(y - self.sigmoid(X.dot(params))))\n",
    "        return params\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X:np.matrix,y:np.array,params:np.array,iterations:int,alpha:float,sample_size:int=1)->np.array:\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        param: sample_size - batch size\n",
    "        \"\"\"\n",
    "        assert sample_size <= X.shape[0]\n",
    "        df_X = pd.DataFrame(X)\n",
    "        df_y = pd.DataFrame(y)\n",
    "        for i in range(iterations):\n",
    "            n_samples = math.ceil(df_X.shape[0]/sample_size)\n",
    "            shuffled = df_X.sample(frac=1)\n",
    "            samples = np.array_split(shuffled, n_samples)\n",
    "            for sample in samples:\n",
    "                X_st = np.array(sample)\n",
    "                y_st = np.array(y[sample.index])\n",
    "                # y_st = np.expand_dims(y_st, axis=-1)\n",
    "                params = params + alpha * (X_st.T.dot(y_st - self.sigmoid(X_st.dot(params))))\n",
    "        return params\n",
    "    \n",
    "    def irls(self,X:np.matrix,y:np.array,iterations:int=1000)->np.array:\n",
    "        \"\"\"\n",
    "        Performs Iterative-Reweighted Least Squares optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: iterations - number of iterations\n",
    "        \"\"\"\n",
    "        w = np.zeros((X.shape[1],1))\n",
    "        for i in range(iterations):\n",
    "            y_ = self.sigmoid(np.matmul(X,w))\n",
    "            R = np.diag(np.ravel(y_*(1-y_)))\n",
    "            grad = np.matmul(X.T,(y_-y))\n",
    "            hessian = np.matmul(np.matmul(X.T,R),X)+0.001*np.eye(X.shape[1])\n",
    "            w -= np.matmul(np.linalg.inv(hessian),grad)\n",
    "        return w\n",
    "    \n",
    "    def adam(self,X:np.matrix,y:np.array,b1:float,b2:float,iterations:int,alpha:float,eps:float)->np.array:\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector does not \n",
    "        contain intercept - it is a separate variable (named b)\n",
    "        and the design matrix does not include additional \n",
    "        one column\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        param: sample_size - batch size\n",
    "        param: b - intercept (optional)\n",
    "        param: b1, b2 - initial decay rates used when estimating \n",
    "            the first and second moments of the gradient\n",
    "        \"\"\"\n",
    "        m,n = X.shape\n",
    "        W,b = np.random.randn(n,1),np.random.randn(1)\n",
    "        VW,Vb = np.zeros((n,1)),np.zeros(1)\n",
    "        SW,Sb = np.zeros((n,1)),np.zeros(1)\n",
    "        \n",
    "        y = y.reshape(len(y),1)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            Z = X.dot(W)+b\n",
    "            # sigmoid\n",
    "            A = self.sigmoid(-Z)\n",
    "            \n",
    "            # binary classification cost\n",
    "            j = (-y*np.log(A)- (1-y)*np.log(1-A)).sum()*(1/m)  # constant value (1/m)\n",
    "\n",
    "            # derivative respect to j\n",
    "            dA = (A-y)/(A*(1-A))\n",
    "            dZ = A-y\n",
    "\n",
    "            dW = X.transpose().dot(dZ)\n",
    "            db = dZ.sum()\n",
    "            \n",
    "            # momentum\n",
    "            VW = b1*VW + (1-b1)*dW\n",
    "            Vb = b1*Vb + (1-b1)*db\n",
    "            \n",
    "            # rmsprop\n",
    "            SW = b2*SW + (1-b2)*dW**2\n",
    "            Sb = b2*Sb + (1-b2)*db**2\n",
    "            \n",
    "            # update weight\n",
    "            W -= alpha*VW/(np.sqrt(SW)+eps)\n",
    "            b -= alpha*Vb/(np.sqrt(Sb)+eps)\n",
    "        return W,b,A  # pred_labels = np.round(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train_x = pd.read_csv(\"../datasets/preprocessed/credit_train_x.csv\")\n",
    "credit_train_y = pd.read_csv(\"../datasets/preprocessed/credit_train_y.csv\")\n",
    "\n",
    "credit_train_x = np.array(credit_train_x)\n",
    "credit_train_y = np.array(credit_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final, y_final = credit_train_x, credit_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative-Reweighted Least Squares\n",
      "\n",
      "Time eclapsed for IRLS 0.11946773529052734 secs\n",
      "Initial cost  519.860385419959\n",
      "Final cost  329.8412760667467\n",
      "\n",
      "\n",
      "\n",
      "Gradient Descent\n",
      "\n",
      "Time eclapsed for GD is 0.1935269832611084 secs\n",
      "Initial cost  1221.595277155707\n",
      "Final cost  456.18796219883654\n",
      "\n",
      "\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "\n",
      "Time eclapsed for SGD 399.4033818244934 secs\n",
      "Initial cost  1091.960263018058\n",
      "Final cost  412.82213784954996\n",
      "\n",
      "\n",
      "\n",
      "ADAM\n",
      "\n",
      "Time eclapsed for ADAM 6.4455108642578125 secs\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameters such as iterations\n",
    "iterations = 10\n",
    "# Creating the instance of the class\n",
    "optimizer = Optimizers()\n",
    "\n",
    "# Iterative-Reweighted Least Squares\n",
    "start = time.time()\n",
    "params_optimal_irls  = optimizer.irls(X_final,y_final,iterations=iterations)\n",
    "end = time.time()\n",
    "params_irls = np.zeros((X_final.shape[1],1))\n",
    "initial_cost_irls = optimizer.cost_function(X_final, y_final,params_irls)\n",
    "final_cost_irls = optimizer.cost_function(X_final, y_final,params_optimal_irls)\n",
    "\n",
    "print('Iterative-Reweighted Least Squares\\n')\n",
    "print(f'Time eclapsed for IRLS {end-start} secs')\n",
    "print('Initial cost ',initial_cost_irls)\n",
    "print('Final cost ',final_cost_irls)\n",
    "print('\\n\\n')\n",
    "\n",
    "# Gradient descent\n",
    "# Defining the parameters such as alpha, iterations\n",
    "iterations = 2000\n",
    "learning_rate = 2e-5\n",
    "\n",
    "params_gd = np.random.randn(X_final.shape[1])\n",
    "params_gd = params_gd[:,np.newaxis]\n",
    "\n",
    "start = time.time()\n",
    "intial_cost_gd = optimizer.cost_function(X_final,y_final,params_gd)\n",
    "params_optimal_gd  = optimizer.gradient_descent(X_final,y_final,params_gd,iterations,learning_rate)\n",
    "end = time.time()\n",
    "final_cost_gd = optimizer.cost_function(X_final,y_final,params_optimal_gd)\n",
    "\n",
    "print('Gradient Descent\\n')\n",
    "print(f'Time eclapsed for GD is {end-start} secs')\n",
    "print('Initial cost ',intial_cost_gd)\n",
    "print('Final cost ',final_cost_gd)\n",
    "print('\\n\\n')\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "iterations = 2000\n",
    "learning_rate = 2e-5\n",
    "params_sgd = np.random.randn(X_final.shape[1])\n",
    "params_sgd = params_sgd[:,np.newaxis]\n",
    "intial_cost_sgd = optimizer.cost_function(X_final, y_final, params_sgd)\n",
    "\n",
    "start = time.time()\n",
    "params_optimal_sgd  = optimizer.stochastic_gradient_descent(X_final, y_final, params_sgd, iterations, learning_rate)\n",
    "end = time.time()\n",
    "final_cost_sgd = optimizer.cost_function(X_final, y_final, params_optimal_sgd)\n",
    "\n",
    "print('Stochastic Gradient Descent\\n')\n",
    "print(f'Time eclapsed for SGD {end-start} secs')\n",
    "print('Initial cost ',intial_cost_sgd)\n",
    "print('Final cost ',final_cost_sgd)\n",
    "print('\\n\\n')\n",
    "\n",
    "# ADAM\n",
    "b1=0.9\n",
    "b2=0.999\n",
    "iterations=20000\n",
    "alpha=1e-6\n",
    "eps=1e-8\n",
    "\n",
    "start = time.time()\n",
    "params_optimal_adam,b,A = optimizer.adam(X_final,y_final,b1=0.9,b2=0.999,iterations=20000,alpha=1e-6,eps=1e-8)\n",
    "end = time.time()\n",
    "\n",
    "print('ADAM\\n')\n",
    "print(f'Time eclapsed for ADAM {end-start} secs')\n",
    "print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
