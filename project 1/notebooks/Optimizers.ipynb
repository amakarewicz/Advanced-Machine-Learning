{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from numpy.linalg import inv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources for IWLS:\n",
    "# https://datascience.stackexchange.com/questions/75166/iterative-reweighted-least-squares-in-python\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb\n",
    "\n",
    "# Sources for GD & SGD:\n",
    "# https://stackoverflow.com/questions/47795918/logistic-regression-gradient-descent\n",
    "# https://medium.com/analytics-vidhya/gradient-descent-and-stochastic-gradient-descent-from-scratch-python-1cd93d4def49\n",
    "# https://github.com/Darshansol9/GD-SGD_FromScratch_Python/blob/master/Code.ipynb\n",
    "\n",
    "# Sources for ADAM:\n",
    "# https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d\n",
    "# https://github.com/jiexunsee/Adam-Optimizer-from-scratch/blob/master/ADAM.ipynb\n",
    "# https://machinelearningmastery.com/adam-optimization-from-scratch/\n",
    "\n",
    "# Source for cross-validation:\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers:\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        # Activation function used to map any real value between 0 and 1\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def cost_function(self,params,X,y):\n",
    "        # Computes the cost function for all the training samples\n",
    "        fx = self.sigmoid(X.dot(params))\n",
    "        cost = -np.sum(y * np.log(fx) + (1 - y)* np.log(1-fx))\n",
    "        return cost\n",
    "    \n",
    "    def cost_derivative(self,params,X,y):\n",
    "        m=X.shape[0]\n",
    "        dfx=(1/m)*X.T.dot(self.sigmoid(X.dot(params))-y)\n",
    "        return dfx\n",
    "        \n",
    "    \n",
    "    def gradient_descent(self,params,X,y,iterations,alpha):\n",
    "        for i in range(iterations):\n",
    "            params = params + alpha * (X.T.dot(y - self.sigmoid(X.dot(params))))\n",
    "        return params\n",
    "    \n",
    "    def stochastic_gradient_descent(self,params,X,y,iterations,alpha,sample_size=1):\n",
    "        assert sample_size <= X.shape[0]\n",
    "        df_X = pd.DataFrame(X)\n",
    "        df_y = pd.DataFrame(y)\n",
    "        for i in range(iterations):\n",
    "            n_samples = math.ceil(df_X.shape[0]/sample_size)\n",
    "            shuffled = df_X.sample(frac=1)\n",
    "            samples = np.array_split(shuffled, n_samples)\n",
    "            for sample in samples:\n",
    "                X_st = np.array(sample)\n",
    "                y_st = np.array(y[sample.index])\n",
    "                # y_st = np.expand_dims(y_st, axis=-1)\n",
    "                params = params + alpha * (X_st.T.dot(y_st - self.sigmoid(X_st.dot(params))))\n",
    "        return params\n",
    "    \n",
    "    def irls(self,X,y,iterations=1000):\n",
    "        \"\"\"\n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising Boolean value\n",
    "        \"\"\"\n",
    "        w = np.zeros((X.shape[1],1))\n",
    "        for i in range(iterations):\n",
    "            y_ = self.sigmoid(np.matmul(X,w))\n",
    "            R = np.diag(np.ravel(y_*(1-y_)))\n",
    "            grad = np.matmul(X.T,(y_-y))\n",
    "            hessian = np.matmul(np.matmul(X.T,R),X)+0.001*np.eye(X.shape[1])\n",
    "            w -= np.matmul(np.linalg.inv(hessian),grad)\n",
    "        return w\n",
    "    \n",
    "    def adam(self,X,y,b1,b2,iterations,alpha,eps):\n",
    "        m,n = X.shape\n",
    "        W,b = np.random.randn(n,1),np.random.randn(1)\n",
    "        VW,Vb = np.zeros((n,1)),np.zeros(1)\n",
    "        SW,Sb = np.zeros((n,1)),np.zeros(1)\n",
    "        \n",
    "        y = y.reshape(len(y),1)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            Z = X.dot(W)+b\n",
    "            # sigmoid\n",
    "            A = self.sigmoid(-Z)\n",
    "            \n",
    "            # binary classification cost\n",
    "            j = (-y*np.log(A)- (1-y)*np.log(1-A)).sum()*(1/m)  # constant value (1/m)\n",
    "\n",
    "            # derivative respect to j\n",
    "            dA = (A-y)/(A*(1-A))\n",
    "            dZ = A-y\n",
    "\n",
    "            dW = X.transpose().dot(dZ)\n",
    "            db = dZ.sum()\n",
    "            \n",
    "            # momentum\n",
    "            VW = b1*VW + (1-b1)*dW\n",
    "            Vb = b1*Vb + (1-b1)*db\n",
    "            \n",
    "            # rmsprop\n",
    "            SW = b2*SW + (1-b2)*dW**2\n",
    "            Sb = b2*Sb + (1-b2)*db**2\n",
    "            \n",
    "            # update weight\n",
    "            W -= alpha*VW/(np.sqrt(SW)+eps)\n",
    "            b -= alpha*Vb/(np.sqrt(Sb)+eps)\n",
    "        return W,b,A  # pred_labels = np.round(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_train_x = pd.read_csv(\"../datasets/preprocessed/credit_train_x.csv\")\n",
    "credit_train_y = pd.read_csv(\"../datasets/preprocessed/credit_train_y.csv\")\n",
    "\n",
    "credit_train_x = np.array(credit_train_x)\n",
    "credit_train_y = np.array(credit_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final, y_final = credit_train_x, credit_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative-Reweighted Least Squares\n",
      "\n",
      "Time eclapsed for IRLS 0.05204916000366211 secs\n",
      "Initial cost  519.860385419959\n",
      "Final cost  329.8412760667467\n",
      "\n",
      "\n",
      "\n",
      "Gradient Descent\n",
      "\n",
      "Time eclapsed for GD is 0.047278642654418945 secs\n",
      "Initial cost  1490.6749175945429\n",
      "Final cost  454.35621275904\n",
      "\n",
      "\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "\n",
      "Time eclapsed for SGD 108.01680064201355 secs\n",
      "Initial cost  978.2061742215765\n",
      "Final cost  440.4543264725253\n",
      "\n",
      "\n",
      "\n",
      "ADAM\n",
      "\n",
      "Time eclapsed for ADAM 1.9808518886566162 secs\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameters such as iterations\n",
    "iterations = 10\n",
    "# Creating the instance of the class\n",
    "optimizer = Optimizers()\n",
    "\n",
    "# Iterative-Reweighted Least Squares\n",
    "start = time.time()\n",
    "params_optimal_irls  = optimizer.irls(X_final,y_final,iterations=iterations)\n",
    "end = time.time()\n",
    "params_irls = np.zeros((X_final.shape[1],1))\n",
    "initial_cost_irls = optimizer.cost_function(params_irls, X_final, y_final)\n",
    "final_cost_irls = optimizer.cost_function(params_optimal_irls, X_final, y_final)\n",
    "\n",
    "print('Iterative-Reweighted Least Squares\\n')\n",
    "print(f'Time eclapsed for IRLS {end-start} secs')\n",
    "print('Initial cost ',initial_cost_irls)\n",
    "print('Final cost ',final_cost_irls)\n",
    "print('\\n\\n')\n",
    "\n",
    "# Gradient descent\n",
    "# Defining the parameters such as alpha, iterations\n",
    "iterations = 2000\n",
    "learning_rate = 2e-5\n",
    "\n",
    "params_gd = np.random.randn(X_final.shape[1])\n",
    "params_gd = params_gd[:,np.newaxis]\n",
    "\n",
    "start = time.time()\n",
    "intial_cost_gd = optimizer.cost_function(params_gd,X_final,y_final)\n",
    "params_optimal_gd  = optimizer.gradient_descent(params_gd,X_final,y_final,iterations,learning_rate)\n",
    "end = time.time()\n",
    "final_cost_gd = optimizer.cost_function(params_optimal_gd, X_final, y_final)\n",
    "\n",
    "print('Gradient Descent\\n')\n",
    "print(f'Time eclapsed for GD is {end-start} secs')\n",
    "print('Initial cost ',intial_cost_gd)\n",
    "print('Final cost ',final_cost_gd)\n",
    "print('\\n\\n')\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "iterations = 2000\n",
    "learning_rate = 2e-5\n",
    "params_sgd = np.random.randn(X_final.shape[1])\n",
    "params_sgd = params_sgd[:,np.newaxis]\n",
    "intial_cost_sgd = optimizer.cost_function(params_sgd, X_final, y_final)\n",
    "\n",
    "start = time.time()\n",
    "params_optimal_sgd  = optimizer.stochastic_gradient_descent(params_sgd, X_final, y_final, iterations, learning_rate)\n",
    "end = time.time()\n",
    "final_cost_sgd = optimizer.cost_function(params_optimal_sgd, X_final, y_final)\n",
    "\n",
    "print('Stochastic Gradient Descent\\n')\n",
    "print(f'Time eclapsed for SGD {end-start} secs')\n",
    "print('Initial cost ',intial_cost_sgd)\n",
    "print('Final cost ',final_cost_sgd)\n",
    "print('\\n\\n')\n",
    "\n",
    "# ADAM\n",
    "b1=0.9\n",
    "b2=0.999\n",
    "iterations=20000\n",
    "alpha=1e-6\n",
    "eps=1e-8\n",
    "\n",
    "start = time.time()\n",
    "params_optimal_adam,b,A = optimizer.adam(X_final,y_final,b1=0.9,b2=0.999,iterations=20000,alpha=1e-6,eps=1e-8)\n",
    "end = time.time()\n",
    "\n",
    "print('ADAM\\n')\n",
    "print(f'Time eclapsed for ADAM {end-start} secs')\n",
    "print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
