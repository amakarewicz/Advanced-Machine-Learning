{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.468867Z",
     "start_time": "2022-03-29T21:20:12.355773Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from numpy.linalg import inv\n",
    "from typing import Tuple\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.484336Z",
     "start_time": "2022-03-29T21:20:15.472863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sources for IWLS:\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb\n",
    "# https://github.com/aehaynes/IRLS/blob/master/irls.py\n",
    "\n",
    "# Sources for GD & SGD:\n",
    "# https://stackoverflow.com/questions/47795918/logistic-regression-gradient-descent\n",
    "# https://medium.com/analytics-vidhya/gradient-descent-and-stochastic-gradient-descent-from-scratch-python-1cd93d4def49\n",
    "# https://github.com/Darshansol9/GD-SGD_FromScratch_Python/blob/master/Code.ipynb\n",
    "\n",
    "# Sources for ADAM:\n",
    "# https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d\n",
    "# https://stackoverflow.com/questions/67080049/adam-optimization-for-gradient-descent-update-doesnt-seem-to-work-with-logistic?fbclid=IwAR3uW95_Entc1-esFrQtVhBKvIWE43781OW6OGIYWJAGLOa37o_z_tHpV0Q\n",
    "\n",
    "# Source for cross-validation:\n",
    "# https://github.com/jaeho3690/LogisiticRegression/blob/main/LogisticRegressionIRLS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.531748Z",
     "start_time": "2022-03-29T21:20:15.487328Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\" Custom implementation of logistic regression algorithm\"\"\"\n",
    "    \n",
    "    def __sigmoid(self,x:float)->float:\n",
    "        \"\"\" Activation function used to map any real value between 0 and 1 \"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def loss_function(self,X:np.matrix,y:np.array,params:np.array,b:np.array=None)->float:\n",
    "        \"\"\"\n",
    "        Computes the cost function for all the training samples \n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        \"\"\"\n",
    "        m,_ = X.shape\n",
    "        if b:\n",
    "            fx=self.__sigmoid(X.dot(params)+b)\n",
    "        else:\n",
    "            fx=self.__sigmoid(X.dot(params))\n",
    "        cost=-np.sum(y * np.log(fx) + (1 - y)*np.log(1-fx))*(1/m)\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(self,X:np.matrix,y:np.array,params:np.array,iterations:int,alpha:float,min_delta:float,patience:int)->np.array:\n",
    "        \"\"\"\n",
    "        Performs gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        param: min_delta - minimum loss function change in the monitored quantity to qualify as improvement.\n",
    "        param: patience - number of epochs with no improvement after which training will be stopped\n",
    "        \"\"\"\n",
    "        cost_history = []\n",
    "        # early stopping setup\n",
    "        if min_delta and patience:\n",
    "            prev_loss,monitor = 0,0\n",
    "            X,X_val,y,y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        else:\n",
    "            monitor=iterations\n",
    "\n",
    "        for i in range(iterations):\n",
    "            params = params+alpha*(X.T.dot(y-self.__sigmoid(X.dot(params))))\n",
    "            cost_history.append(self.loss_function(X,y,params))\n",
    "            # early stopping\n",
    "            if min_delta and patience:\n",
    "                loss = self.loss_function(X_val,y_val,params)\n",
    "                cost_change = loss-prev_loss\n",
    "                prev_loss = loss\n",
    "                monitor=monitor+1 if (loss-prev_loss)<min_delta else 0\n",
    "                if monitor==patience: break\n",
    "        return params,monitor,cost_history\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X:np.matrix,y:np.array,params:np.array,iterations:int,alpha:float,min_delta:float,patience:float,sample_size:int=1)->np.array:\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        param: sample_size - batch size\n",
    "        param: min_delta - minimum loss function change in the monitored quantity to qualify as improvement.\n",
    "        param: patience - number of epochs with no improvement after which training will be stopped\n",
    "        \"\"\"\n",
    "        cost_history = []\n",
    "        # early stopping setup\n",
    "        if min_delta and patience:\n",
    "            prev_loss,monitor = 0,0\n",
    "            X,X_val,y,y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        else:\n",
    "            monitor=iterations\n",
    "        \n",
    "        # optimization setup\n",
    "        assert sample_size <= X.shape[0]\n",
    "        df_X = pd.DataFrame(X)\n",
    "        df_y = pd.DataFrame(y)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            n_samples = math.ceil(df_X.shape[0]/sample_size)\n",
    "            shuffled = df_X.sample(frac=1)\n",
    "            samples = np.array_split(shuffled, n_samples)\n",
    "            for sample in samples:\n",
    "                X_st = np.array(sample)\n",
    "                y_st = np.array(y[sample.index])\n",
    "                # y_st = np.expand_dims(y_st, axis=-1)\n",
    "                params = params + alpha * (X_st.T.dot(y_st - self.__sigmoid(X_st.dot(params))))\n",
    "            cost_history.append(self.loss_function(X,y,params))\n",
    "            # early stopping\n",
    "            if min_delta and patience:\n",
    "                loss = self.loss_function(X_val,y_val,params)\n",
    "                cost_change = loss-prev_loss\n",
    "                prev_loss = loss\n",
    "                monitor=monitor+1 if (loss-prev_loss)<min_delta else 0\n",
    "                if monitor==patience: break\n",
    "            \n",
    "        return params,monitor,cost_history\n",
    "    \n",
    "    def irls(self,X:np.matrix,y:np.array,min_delta:float,patience:int,iterations:int=1000)->np.array:\n",
    "        \"\"\"\n",
    "        Performs Iterative-Reweighted Least Squares optimization.\n",
    "        \n",
    "        Works assuming that weights vector contains\n",
    "        intercept and the corresponding one column \n",
    "        has been added to the design matrix before \n",
    "        it is given to the method\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: iterations - number of iterations\n",
    "        param: min_delta - minimum loss function change in the monitored quantity to qualify as improvement.\n",
    "        param: patience - number of epochs with no improvement after which training will be stopped\n",
    "        \"\"\"\n",
    "        cost_history = []\n",
    "        # early stopping setup\n",
    "        if min_delta and patience:\n",
    "            prev_loss,monitor = 0,0\n",
    "            X,X_val,y,y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        else:\n",
    "            monitor=iterations\n",
    "        # optimization setup\n",
    "        params = np.zeros((X.shape[1],1))\n",
    "\n",
    "        for i in range(iterations):\n",
    "            y_ = self.__sigmoid(np.matmul(X,params))\n",
    "            R = np.diag(np.ravel(y_*(1-y_)))\n",
    "            grad = np.matmul(X.T,(y_-y))\n",
    "            hessian = np.matmul(np.matmul(X.T,R),X)+0.001*np.eye(X.shape[1])\n",
    "            params -= np.matmul(np.linalg.inv(hessian),grad)\n",
    "            cost_history.append(self.loss_function(X,y,params))\n",
    "            # early stopping\n",
    "            if min_delta and patience:\n",
    "                loss = self.loss_function(X=X_val,y=y_val,params=params)\n",
    "                cost_change = loss-prev_loss\n",
    "                prev_loss = loss\n",
    "                monitor=monitor+1 if (loss-prev_loss)<min_delta else 0\n",
    "                if monitor==patience:\n",
    "                    break\n",
    "            \n",
    "        return params,monitor,cost_history\n",
    "    \n",
    "    def adam(self,X:np.matrix,y:np.array,b1:float,b2:float,iterations:int,alpha:float,eps:float,min_delta:float,patience:int)->np.array:\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descent optimization.\n",
    "        \n",
    "        Works assuming that weights vector does not \n",
    "        contain intercept - it is a separate variable (named b)\n",
    "        and the design matrix does not include additional \n",
    "        one column\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        param: iterations - number of iterations\n",
    "        param: alpha - learning rate\n",
    "        param: sample_size - batch size\n",
    "        param: b - intercept (optional)\n",
    "        param: b1, b2 - initial decay rates used when estimating \n",
    "            the first and second moments of the gradient\n",
    "        param: min_delta - minimum loss function change in the monitored quantity to qualify as improvement.\n",
    "        param: patience - number of epochs with no improvement after which training will be stopped\n",
    "        \"\"\"\n",
    "        cost_history = []\n",
    "        # early stopping setup\n",
    "        if min_delta and patience:\n",
    "            prev_loss,monitor = 0,0\n",
    "            X,X_val,y,y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        else:\n",
    "            monitor=iterations\n",
    "        \n",
    "        # optimization setup\n",
    "        m,n = X.shape\n",
    "        W,b = np.random.randn(n,1),np.random.randn(1)\n",
    "        VW,Vb = np.zeros((n,1)),np.zeros(1)\n",
    "        SW,Sb = np.zeros((n,1)),np.zeros(1)\n",
    "        prev_loss,monitor = 0,0\n",
    "        y = y.reshape(len(y),1)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # sigmoid\n",
    "            A = self.__sigmoid((X.dot(W)+b))\n",
    "    \n",
    "            # binary classification cost\n",
    "            j = self.loss_function(X=X,y=y,params=W,b=b)  # (-y*np.log(A)-(1-y)*np.log(1-A)).sum()*(1/m)\n",
    "\n",
    "            # derivative respect to j\n",
    "            dA = (A-y)/(A*(1-A))\n",
    "            dZ = A-y\n",
    "\n",
    "            dW = X.transpose().dot(dZ)\n",
    "            db = dZ.sum()\n",
    "            \n",
    "            # momentum\n",
    "            VW = b1*VW + (1-b1)*dW\n",
    "            Vb = b1*Vb + (1-b1)*db\n",
    "            \n",
    "            # rmsprop\n",
    "            SW = b2*SW + (1-b2)*dW**2\n",
    "            Sb = b2*Sb + (1-b2)*db**2\n",
    "            \n",
    "            # update weight\n",
    "            W -= alpha*VW/(np.sqrt(SW)+eps)\n",
    "            b -= alpha*Vb/(np.sqrt(Sb)+eps)\n",
    "            cost_history.append(self.loss_function(X,y,W,b))\n",
    "            # early stopping\n",
    "            if min_delta and patience:\n",
    "                loss = self.loss_function(X=X_val,y=y_val,params=W,b=b)\n",
    "                cost_change = loss-prev_loss\n",
    "                prev_loss = loss\n",
    "                monitor=monitor+1 if (loss-prev_loss)<min_delta else 0\n",
    "                if monitor==patience:\n",
    "                    break\n",
    "            \n",
    "        return W,b,monitor,cost_history\n",
    "    \n",
    "    \n",
    "    def fit(self,X:np.matrix,y:np.array,verbose=False,**kwds)->np.array:\n",
    "        \"\"\"\n",
    "        Fits the model according to the given training data.\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: y - target vector comprising boolean value\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(kwds.keys()) == 4:\n",
    "            algorithm=\"Gradient Descent\"\n",
    "            # parameters setting\n",
    "            # X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            params = np.random.randn(X.shape[1])\n",
    "            params = params[:,np.newaxis]\n",
    "            iterations = kwds[\"iterations\"]\n",
    "            learning_rate = kwds[\"alpha\"]\n",
    "            min_delta = kwds[\"min_delta\"]\n",
    "            patience = kwds[\"patience\"]\n",
    "            # optimization\n",
    "            initial_cost = self.loss_function(X,y,params)\n",
    "            start = time.time()\n",
    "            params_optimal,monitor,cost_history = self.gradient_descent(X,y,params,iterations,learning_rate,min_delta,patience)\n",
    "            end = time.time()\n",
    "            final_cost = self.loss_function(X, y, params_optimal)\n",
    "            \n",
    "        elif len(kwds.keys()) == 5:\n",
    "            algorithm=\"Stochastic Gradient Descent\"\n",
    "            # parameters setting\n",
    "            # X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            params = np.random.randn(X.shape[1])\n",
    "            params = params[:,np.newaxis]\n",
    "            iterations = kwds[\"iterations\"]\n",
    "            learning_rate = kwds[\"alpha\"]\n",
    "            sample_size = kwds[\"sample_size\"]\n",
    "            min_delta = kwds[\"min_delta\"]\n",
    "            patience = kwds[\"patience\"]\n",
    "            # optimization\n",
    "            initial_cost = self.loss_function(X=X,y=y,params=params)\n",
    "            start = time.time()\n",
    "            params_optimal,monitor,cost_history = self.stochastic_gradient_descent(X,y,params,iterations,learning_rate,min_delta,patience,sample_size)\n",
    "            end = time.time()\n",
    "            final_cost = self.loss_function(X=X,y=y,params=params_optimal)\n",
    "            \n",
    "        elif len(kwds.keys()) == 7:\n",
    "            algorithm='ADAM'\n",
    "            # parameters setting\n",
    "            params=np.random.randn(X.shape[1],1)\n",
    "            b1=kwds[\"b1\"]\n",
    "            b2=kwds[\"b2\"]\n",
    "            iterations=kwds[\"iterations\"]\n",
    "            learning_rate=kwds[\"alpha\"]\n",
    "            eps=kwds[\"epsilon\"]\n",
    "            min_delta = kwds[\"min_delta\"]\n",
    "            patience = kwds[\"patience\"]\n",
    "            # optimization\n",
    "            m,n = X.shape\n",
    "            W,b = np.random.randn(n,1),np.random.randn(1)\n",
    "            initial_cost=self.loss_function(X=X,y=y,params=W,b=b)\n",
    "            start=time.time()\n",
    "            W,b,monitor,cost_history=self.adam(X,y,b1=b1,b2=b2,iterations=iterations,alpha=learning_rate,eps=eps,min_delta=min_delta,patience=patience)\n",
    "            end=time.time()\n",
    "            final_cost=self.loss_function(X=X,y=y,params=W,b=b)\n",
    "            params_optimal=(W,b)\n",
    "        else:\n",
    "            algorithm='Iterative-Reweighted Least Squares'\n",
    "            \n",
    "            # X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            # parameters setting\n",
    "            try:\n",
    "                min_delta = kwds[\"min_delta\"]\n",
    "                patience = kwds[\"patience\"]\n",
    "            except Exception as e:\n",
    "                min_delta,patience,monitor = None,None,None\n",
    "                \n",
    "            try:\n",
    "                iterations=kwds[\"iterations\"]\n",
    "                start = time.time()\n",
    "                # optimization\n",
    "                params_optimal,monitor,cost_history = self.irls(X,y,min_delta,patience,iterations=iterations)\n",
    "                end = time.time()\n",
    "            except Exception as e:\n",
    "                start = time.time()\n",
    "                # optimization\n",
    "                params_optimal,monitor,cost_history = self.irls(X,y,min_delta,patience)\n",
    "                end = time.time()\n",
    "            params = np.zeros((X.shape[1],1))\n",
    "            initial_cost = self.loss_function(X=X,y=y,params=params)\n",
    "            final_cost = self.loss_function(X=X,y=y,params=params_optimal)\n",
    "        if verbose:\n",
    "            display(Markdown(f'### {algorithm}\\n'))\n",
    "            print(f'Fitting stopped after {monitor} iterations')\n",
    "            print(f'Time eclapsed for fitting: {end-start} secs')\n",
    "            print('Initial cost ',initial_cost)\n",
    "            print('Final cost ',final_cost)\n",
    "            print('\\n\\n')\n",
    "        \n",
    "        return params_optimal,monitor,final_cost,cost_history\n",
    "    \n",
    "    def predict(self,X:np.matrix,params:np.array,b:np.array=None,threshold=.5)->Tuple[np.array,np.array]:\n",
    "        \"\"\"\n",
    "        Predicts probability estimates and labels for samples in X.\n",
    "        \n",
    "        param: X - design matrix\n",
    "        param: params - array of weights\n",
    "        param: b - intercept (optional)\n",
    "        param: threshold - probability threshold. If the probability estimate\n",
    "            returned for a given observation exceeds its value, the observation\n",
    "            is assigned to the positive class\n",
    "        \"\"\"\n",
    "        if b:\n",
    "            prob_pred=self.__sigmoid(X.dot(params)+b)\n",
    "        else:\n",
    "            # X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            prob_pred=self.__sigmoid(X.dot(params))\n",
    "        y_pred = (prob_pred > threshold).astype(int)\n",
    "        y_pred = [p for pred in y_pred for p in pred]\n",
    "        return prob_pred, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.562873Z",
     "start_time": "2022-03-29T21:20:15.533693Z"
    }
   },
   "outputs": [],
   "source": [
    "credit_train_x = pd.read_csv(\"../datasets/preprocessed/credit_train_x.csv\")\n",
    "credit_train_y = pd.read_csv(\"../datasets/preprocessed/credit_train_y.csv\")\n",
    "\n",
    "credit_train_x = np.array(credit_train_x)\n",
    "credit_train_y = np.array(credit_train_y)\n",
    "\n",
    "X, y = credit_train_x, credit_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.909954Z",
     "start_time": "2022-03-29T21:20:15.563870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iterative-Reweighted Least Squares\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stopped after 100 iterations\n",
      "Time eclapsed for fitting: 3.432231903076172 secs\n",
      "Initial cost  0.6931471805599453\n",
      "Final cost  0.43978836808899546\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8394666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterative-Reweighted Least Squares\n",
    "iterations=100\n",
    "min_delta=None\n",
    "patience = None\n",
    "lr = LogisticRegression()\n",
    "kwds={\n",
    "    \"iterations\": iterations,\n",
    "    \"min_delta\":min_delta,\n",
    "    \"patience\":patience\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X_ = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "W,n_iter,final_cost,cost_history=lr.fit(X_,y,verbose=True,**kwds)\n",
    "prob_pred,y_pred=lr.predict(X_, W)\n",
    "\n",
    "roc_auc_score(y, prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:15.941868Z",
     "start_time": "2022-03-29T21:20:15.911949Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Gradient Descent\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stopped after 3 iterations\n",
      "Time eclapsed for fitting: 0.01664137840270996 secs\n",
      "Initial cost  1.4279422302972695\n",
      "Final cost  1.4190768904796287\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6112846560846561"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent\n",
    "iterations = 1000\n",
    "learning_rate = 2e-5\n",
    "min_delta=0.01\n",
    "patience = 3\n",
    "\n",
    "gd_kwds={\n",
    "    \"iterations\": iterations,\n",
    "    \"alpha\": learning_rate,\n",
    "    \"min_delta\":min_delta,\n",
    "    \"patience\":patience\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X_ = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "W,n_iter,final_cost,cost_history=lr.fit(X_,y,verbose=True,**gd_kwds)\n",
    "prob_pred,y_pred=lr.predict(X_, W)\n",
    "\n",
    "roc_auc_score(y, prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:16.210158Z",
     "start_time": "2022-03-29T21:20:15.942866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Stochastic Gradient Descent\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stopped after 3 iterations\n",
      "Time eclapsed for fitting: 0.5294439792633057 secs\n",
      "Initial cost  1.633865785710079\n",
      "Final cost  1.6029804249389101\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4753523809523809"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic gradient descent\n",
    "iterations = 2000\n",
    "learning_rate = 2e-5\n",
    "sample_size = 1\n",
    "min_delta=0.01\n",
    "patience = 3\n",
    "\n",
    "sgd_kwds={\n",
    "    \"iterations\": iterations,\n",
    "    \"alpha\": learning_rate,\n",
    "    \"sample_size\": sample_size,\n",
    "    \"min_delta\":min_delta,\n",
    "    \"patience\":patience\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "X_ = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "W,n_iter,final_cost,cost_history=lr.fit(X_,y,verbose=True,**sgd_kwds)\n",
    "prob_pred,y_pred=lr.predict(X_, W)\n",
    "\n",
    "roc_auc_score(y, prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:16.226115Z",
     "start_time": "2022-03-29T21:20:16.212152Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ADAM\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stopped after 3 iterations\n",
      "Time eclapsed for fitting: 0.013057947158813477 secs\n",
      "Initial cost  2.1102494968112167\n",
      "Final cost  1.4386024891233222\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5955216931216932"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADAM\n",
    "b1=0.9\n",
    "b2=0.999\n",
    "iterations=20000\n",
    "alpha=2e-5\n",
    "eps=1e-8\n",
    "min_delta=0.01\n",
    "patience = 3\n",
    "\n",
    "adam_kwds={\n",
    "    \"iterations\": iterations,\n",
    "    \"b1\": b1,\n",
    "    \"b2\": b2,\n",
    "    \"alpha\": alpha,\n",
    "    \"epsilon\": eps,\n",
    "    \"min_delta\":min_delta,\n",
    "    \"patience\":patience\n",
    "}\n",
    "lr = LogisticRegression()\n",
    "params_optimal,n_iter,final_cost,cost_history=lr.fit(X,y,verbose=True,**adam_kwds)\n",
    "W,b=params_optimal\n",
    "prob_pred,y_pred=lr.predict(X=X,params=W,b=b)\n",
    "\n",
    "roc_auc_score(y, prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T21:20:19.196178Z",
     "start_time": "2022-03-29T21:20:16.227113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iterative-Reweighted Least Squares\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting stopped after 1000 iterations\n",
      "Time eclapsed for fitting: 28.603841066360474 secs\n",
      "Initial cost  0.6931471805599453\n",
      "Final cost  0.4397883680889956\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8394666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterative-Reweighted Least Squares\n",
    "lr = LogisticRegression()\n",
    "X_ = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "W,n_iter,final_cost,cost_history=lr.fit(X_,y,verbose=True)\n",
    "prob_pred,y_pred=lr.predict(X_, W)\n",
    "\n",
    "roc_auc_score(y, prob_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4650787503170151,\n",
       " 0.44184257176635405,\n",
       " 0.4398151466676699,\n",
       " 0.43978837450275804,\n",
       " 0.4397883680889969,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.43978836808899546,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.43978836808899546,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956,\n",
       " 0.4397883680889956]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
