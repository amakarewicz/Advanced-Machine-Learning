{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889107a6",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "*Elżbieta Jowik* <br>\n",
    "*Agata Makarewicz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4542708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "plt.rcParams['figure.figsize'] = (9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd65462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def evaluate_classification(true, pred):\n",
    "    Accuracy = accuracy_score(true, pred)\n",
    "    Precision = precision_score(true, pred)\n",
    "    Recall = recall_score(true, pred)\n",
    "    F1_score = f1_score(true, pred)\n",
    "    \n",
    "    results = pd.DataFrame(np.array([Accuracy, Precision, Recall, F1_score]))\n",
    "    results.index = ['Accuracy', 'Precision', 'Recall', 'F1_score']\n",
    "    return results\n",
    "\n",
    "# target_names = list(np.unique(data_target))\n",
    "\n",
    "# def evaluate_metrics(true, pred):\n",
    "#     Precision = precision_score(true, pred, average = None)\n",
    "#     Recall = recall_score(true, pred, average = None)\n",
    "#     F1_score = f1_score(true, pred, average = None)\n",
    "#     F_beta = fbeta_score(true, pred, average = None,beta=2)\n",
    "    \n",
    "#     results = pd.DataFrame(np.array([Precision,Recall,F1_score,F_beta]))\n",
    "#     results.index = ['Precision','Recall','F1_score','F_beta']\n",
    "#     results.columns = target_names\n",
    "#     f1 = f1_score(true, pred, average='weighted')\n",
    "#     return results, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7326cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading preprocessed datasets\n",
    "data_adult_train = pd.read_csv('datasets/project1/preprocessed/adult_train_x.csv')\n",
    "data_adult_test = pd.read_csv('datasets/project1/preprocessed/adult_test_x.csv')\n",
    "\n",
    "data_credit_train = pd.read_csv('datasets/project1/preprocessed/credit_train_x.csv')\n",
    "data_credit_test = pd.read_csv('datasets/project1/preprocessed/credit_test_x.csv')\n",
    "\n",
    "data_sick_train = pd.read_csv('datasets/project1/preprocessed/sick_train_x.csv')\n",
    "data_sick_test = pd.read_csv('datasets/project1/preprocessed/sick_test_x.csv')\n",
    "\n",
    "data_titanic_train = pd.read_csv('datasets/project1/preprocessed/titanic_train_x.csv')\n",
    "data_titanic_test = pd.read_csv('datasets/project1/preprocessed/titanic_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf15acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading preprocessed datasets\n",
    "target_adult_train = pd.read_csv('datasets/project1/preprocessed/adult_train_y.csv')\n",
    "target_adult_test = pd.read_csv('datasets/project1/preprocessed/adult_test_y.csv')\n",
    "\n",
    "target_credit_train = pd.read_csv('datasets/project1/preprocessed/credit_train_y.csv')\n",
    "target_credit_test = pd.read_csv('datasets/project1/preprocessed/credit_test_y.csv')\n",
    "\n",
    "target_sick_train = pd.read_csv('datasets/project1/preprocessed/sick_train_y.csv')\n",
    "target_sick_test = pd.read_csv('datasets/project1/preprocessed/sick_test_y.csv')\n",
    "\n",
    "target_titanic_train = pd.read_csv('datasets/project1/preprocessed/titanic_train_y.csv')\n",
    "target_titanic_test = pd.read_csv('datasets/project1/preprocessed/titanic_test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f02cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(true, pred):\n",
    "    correct = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(true))\n",
    "\n",
    "# def precision_metric(true, pred):\n",
    "    \n",
    "# def recall_metric(true, pred):\n",
    "\n",
    "# def f_metric(true, pred):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a758237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################   LINKI !!!   #################################################\n",
    "# https://github.com/pysal/spglm\n",
    "# https://dphi.tech/blog/tutorial-on-logistic-regression-using-python/\n",
    "# https://github.com/PhongHoangg/Gradient-Descent-for-Logistics-Regression/blob/main/Gradient%20Descent%20for%20Logistics%20Regression.ipynb\n",
    "# https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\n",
    "# https://www.analyticsvidhya.com/blog/2021/05/how-can-we-implement-logistic-regression/\n",
    "# https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24\n",
    "# https://github.com/theroyakash/Adam/blob/master/Code/Adam.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11bb7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iwls_optimizer():\n",
    "\n",
    "# def gd_optimizer():\n",
    "    \n",
    "# def sgd_optimizer():\n",
    "    \n",
    "# def adam_optimizer():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b61ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef\n",
    "\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    "\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc6193b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.100, error=728.585\n",
      ">epoch=1, lrate=0.100, error=346.315\n",
      ">epoch=2, lrate=0.100, error=214.156\n",
      ">epoch=3, lrate=0.100, error=148.019\n",
      ">epoch=4, lrate=0.100, error=110.527\n",
      ">epoch=5, lrate=0.100, error=87.095\n",
      ">epoch=6, lrate=0.100, error=71.321\n",
      ">epoch=7, lrate=0.100, error=60.089\n",
      ">epoch=8, lrate=0.100, error=51.738\n",
      ">epoch=9, lrate=0.100, error=45.313\n",
      ">epoch=10, lrate=0.100, error=40.235\n",
      ">epoch=11, lrate=0.100, error=36.130\n",
      ">epoch=12, lrate=0.100, error=32.748\n",
      ">epoch=13, lrate=0.100, error=29.919\n",
      ">epoch=14, lrate=0.100, error=27.521\n",
      ">epoch=15, lrate=0.100, error=25.463\n",
      ">epoch=16, lrate=0.100, error=23.680\n",
      ">epoch=17, lrate=0.100, error=22.120\n",
      ">epoch=18, lrate=0.100, error=20.747\n",
      ">epoch=19, lrate=0.100, error=19.527\n",
      ">epoch=20, lrate=0.100, error=18.438\n",
      ">epoch=21, lrate=0.100, error=17.460\n",
      ">epoch=22, lrate=0.100, error=16.577\n",
      ">epoch=23, lrate=0.100, error=15.776\n",
      ">epoch=24, lrate=0.100, error=15.047\n",
      ">epoch=25, lrate=0.100, error=14.380\n",
      ">epoch=26, lrate=0.100, error=13.768\n",
      ">epoch=27, lrate=0.100, error=13.204\n",
      ">epoch=28, lrate=0.100, error=12.683\n",
      ">epoch=29, lrate=0.100, error=12.200\n",
      ">epoch=30, lrate=0.100, error=11.752\n",
      ">epoch=31, lrate=0.100, error=11.335\n",
      ">epoch=32, lrate=0.100, error=10.945\n",
      ">epoch=33, lrate=0.100, error=10.581\n",
      ">epoch=34, lrate=0.100, error=10.239\n",
      ">epoch=35, lrate=0.100, error=9.918\n",
      ">epoch=36, lrate=0.100, error=9.617\n",
      ">epoch=37, lrate=0.100, error=9.332\n",
      ">epoch=38, lrate=0.100, error=9.064\n",
      ">epoch=39, lrate=0.100, error=8.810\n",
      ">epoch=40, lrate=0.100, error=8.569\n",
      ">epoch=41, lrate=0.100, error=8.341\n",
      ">epoch=42, lrate=0.100, error=8.125\n",
      ">epoch=43, lrate=0.100, error=7.919\n",
      ">epoch=44, lrate=0.100, error=7.723\n",
      ">epoch=45, lrate=0.100, error=7.536\n",
      ">epoch=46, lrate=0.100, error=7.358\n",
      ">epoch=47, lrate=0.100, error=7.188\n",
      ">epoch=48, lrate=0.100, error=7.026\n",
      ">epoch=49, lrate=0.100, error=6.870\n",
      ">epoch=50, lrate=0.100, error=6.722\n",
      ">epoch=51, lrate=0.100, error=6.579\n",
      ">epoch=52, lrate=0.100, error=6.442\n",
      ">epoch=53, lrate=0.100, error=6.310\n",
      ">epoch=54, lrate=0.100, error=6.184\n",
      ">epoch=55, lrate=0.100, error=6.063\n",
      ">epoch=56, lrate=0.100, error=5.946\n",
      ">epoch=57, lrate=0.100, error=5.833\n",
      ">epoch=58, lrate=0.100, error=5.725\n",
      ">epoch=59, lrate=0.100, error=5.620\n",
      ">epoch=60, lrate=0.100, error=5.519\n",
      ">epoch=61, lrate=0.100, error=5.422\n",
      ">epoch=62, lrate=0.100, error=5.328\n",
      ">epoch=63, lrate=0.100, error=5.237\n",
      ">epoch=64, lrate=0.100, error=5.149\n",
      ">epoch=65, lrate=0.100, error=5.064\n",
      ">epoch=66, lrate=0.100, error=4.981\n",
      ">epoch=67, lrate=0.100, error=4.901\n",
      ">epoch=68, lrate=0.100, error=4.824\n",
      ">epoch=69, lrate=0.100, error=4.749\n",
      ">epoch=70, lrate=0.100, error=4.676\n",
      ">epoch=71, lrate=0.100, error=4.606\n",
      ">epoch=72, lrate=0.100, error=4.537\n",
      ">epoch=73, lrate=0.100, error=4.471\n",
      ">epoch=74, lrate=0.100, error=4.406\n",
      ">epoch=75, lrate=0.100, error=4.343\n",
      ">epoch=76, lrate=0.100, error=4.282\n",
      ">epoch=77, lrate=0.100, error=4.223\n",
      ">epoch=78, lrate=0.100, error=4.165\n",
      ">epoch=79, lrate=0.100, error=4.108\n",
      ">epoch=80, lrate=0.100, error=4.054\n",
      ">epoch=81, lrate=0.100, error=4.000\n",
      ">epoch=82, lrate=0.100, error=3.948\n",
      ">epoch=83, lrate=0.100, error=3.897\n",
      ">epoch=84, lrate=0.100, error=3.848\n",
      ">epoch=85, lrate=0.100, error=3.800\n",
      ">epoch=86, lrate=0.100, error=3.753\n",
      ">epoch=87, lrate=0.100, error=3.707\n",
      ">epoch=88, lrate=0.100, error=3.662\n",
      ">epoch=89, lrate=0.100, error=3.618\n",
      ">epoch=90, lrate=0.100, error=3.575\n",
      ">epoch=91, lrate=0.100, error=3.533\n",
      ">epoch=92, lrate=0.100, error=3.493\n",
      ">epoch=93, lrate=0.100, error=3.453\n",
      ">epoch=94, lrate=0.100, error=3.414\n",
      ">epoch=95, lrate=0.100, error=3.375\n",
      ">epoch=96, lrate=0.100, error=3.338\n",
      ">epoch=97, lrate=0.100, error=3.302\n",
      ">epoch=98, lrate=0.100, error=3.266\n",
      ">epoch=99, lrate=0.100, error=3.231\n"
     ]
    }
   ],
   "source": [
    "predicted = logistic_regression(data_adult_train, data_adult_test, l_rate = 0.1, n_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d451ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7267113159954771"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(np.array(target_adult_test), predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb5674d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pima-indians-diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# load and prepare data\u001b[39;00m\n\u001b[0;32m    113\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpima-indians-diabetes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 114\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m    116\u001b[0m \tstr_column_to_float(dataset, i)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mload_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_csv\u001b[39m(filename):\n\u001b[0;32m     11\u001b[0m \tdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m---> 12\u001b[0m \t\u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     13\u001b[0m \t\tcsv_reader \u001b[38;5;241m=\u001b[39m reader(file)\n\u001b[0;32m     14\u001b[0m \t\t\u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m csv_reader:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pima-indians-diabetes.csv'"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\n",
    "\n",
    "# Logistic Regression on Diabetes Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "# Split a dataset into k folds -- może sie przydać jakas modyfikacja tego bo chca kroswalidacje w eksperymentach chyba w 4\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e72bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
